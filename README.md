# ğŸŒ² CMPC Data Pipeline: Gastos LogÃ­sticos

![Python](https://img.shields.io/badge/Python-3.10%2B-blue)
![Status](https://img.shields.io/badge/Status-Production-green)
![Data Quality](https://img.shields.io/badge/Data%20Quality-Validated-orange)

## ğŸ“„ DescripciÃ³n del Proyecto

Este proyecto implementa un pipeline de IngenierÃ­a de Datos **ETL (Extract, Transform, Load)** completamente automatizado para la gestiÃ³n y auditorÃ­a de gastos logÃ­sticos de proveedores externos.

El sistema fue diseÃ±ado con una arquitectura modular y defensiva, priorizando la **calidad de datos** y la **observabilidad**.  
Su objetivo es consolidar datos desde fuentes externas (APIs), validarlos mediante reglas de negocio estrictas y almacenarlos en un Data Warehouse local para anÃ¡lisis financiero.

---

## ğŸ—ï¸ Arquitectura del Sistema

El flujo de datos sigue un proceso lineal con *Quality Gates* que detienen la ejecuciÃ³n ante problemas crÃ­ticos:

```mermaid
graph LR
    A[ğŸŒ API Externa] -->|JSON Raw| B(âš¡ Extract)
    B --> C{ğŸ›¡ï¸ Data Quality Check}
    C -->|âœ… Pass| D[âš™ï¸ Transform]
    C -->|âŒ Fail| E[â›” Log Error & Stop]
    D -->|Datos Limpios| F[(ğŸ—„ï¸ SQLite Warehouse)]
    F --> G[ğŸ“Š KPI Reporting]
ğŸ› ï¸ Stack TecnolÃ³gico
Lenguaje: Python 3.x

Procesamiento: Pandas (DataFrames)

Almacenamiento: SQLite + SQLAlchemy

OrquestaciÃ³n: Bash + Cron (Unix Scheduling)

Logging: Sistema de logs rotativos customizado

Control de Versiones: Git & GitHub

ğŸš€ InstalaciÃ³n y ConfiguraciÃ³n
Sigue estos pasos para desplegar el proyecto en un entorno local (macOS/Linux).

1. Clonar el repositorio
bash
Copiar cÃ³digo
git clone https://github.com/Sebapruz/CMPC-Preparacion-Data.git
cd CMPC-Preparacion-Data
2. Configurar el Entorno Virtual
bash
Copiar cÃ³digo
python3 -m venv venv
source venv/bin/activate
3. Instalar Dependencias
bash
Copiar cÃ³digo
pip install -r requirements.txt
ğŸ’» Uso y EjecuciÃ³n
â–¶ï¸ EjecuciÃ³n Manual
Para correr el pipeline completo (incluyendo validaciones):

bash
Copiar cÃ³digo
python3 -m pipelines.etl_robust
Esto generarÃ¡ logs en logs/ y actualizarÃ¡ la base de datos en datasets/.

â° AutomatizaciÃ³n (Cron Job)
El proyecto incluye un Wrapper Script (run_pipeline.sh) diseÃ±ado para orquestar la ejecuciÃ³n mediante Cron.

Dar permisos de ejecuciÃ³n:
bash
Copiar cÃ³digo
chmod +x run_pipeline.sh
Configurar Cron (Ejemplo: Ejecutar todos los dÃ­as a las 09:00 AM):
bash
Copiar cÃ³digo
crontab -e
Agregar la siguiente lÃ­nea (ajustando tu ruta absoluta):

swift
Copiar cÃ³digo
0 9 * * * /ruta/absoluta/a/CMPC-Preparacion-Data/run_pipeline.sh
ğŸ›¡ï¸ Calidad de Datos (Data Quality)
El sistema implementa una capa de ProgramaciÃ³n Defensiva en utils/validations.py.
El pipeline se detendrÃ¡ automÃ¡ticamente (Exit Code 1) si detecta:

Schema Drift: Si la API cambia de formato o faltan columnas obligatorias.

Null Values CrÃ­ticos: Si campos clave como ID o Email vienen vacÃ­os.

Integridad Referencial: Extensible a reglas de negocio especÃ­ficas.

Cualquier incidente de calidad queda registrado con nivel ERROR o CRITICAL en los logs.

ğŸ“‚ Estructura del Proyecto
plaintext
Copiar cÃ³digo
CMPC-Preparacion-Data/
â”œâ”€â”€ config.py           # âš™ï¸ ConfiguraciÃ³n centralizada (Rutas dinÃ¡micas)
â”œâ”€â”€ run_pipeline.sh     # ğŸ¤– Wrapper para automatizaciÃ³n con Cron
â”œâ”€â”€ requirements.txt    # ğŸ“¦ Lista de dependencias
â”œâ”€â”€ datasets/           # ğŸ—„ï¸ Almacenamiento local (Base de Datos)
â”œâ”€â”€ logs/               # ğŸ“ Historial de ejecuciÃ³n (AuditorÃ­a)
â”œâ”€â”€ pipelines/          # ğŸš€ LÃ³gica de Negocio (ETL)
â”‚   â””â”€â”€ etl_robust.py   # Script principal con validaciones
â”œâ”€â”€ utils/              # ğŸ§° Herramientas Reutilizables
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ logger.py       # ConfiguraciÃ³n de logging
â”‚   â””â”€â”€ validations.py  # Motor de reglas de calidad
â””â”€â”€ sql/                # ğŸ” Scripts de anÃ¡lisis y consultas
ğŸ‘¤ Autor
SebastiÃ¡n Palma
Ingeniero de Datos en formaciÃ³n | Enfocado en Arquitecturas Robustas y AutomatizaciÃ³n.

Este proyecto fue desarrollado como parte de una simulaciÃ³n intensiva de IngenierÃ­a de Datos.